v Remove unneeded changes

v params to preProcess (aEOS 1... N/sessions len, gen dir, name Dir)
v Preprocess 1... N
v Preprocess session len digi and recsys

v enable preprocess recsys

v save model        results:  pickle_models: results/next/rsc15/
v load model        class: filemodel.filemodel.FileModel; params: { modelfile: results/next/rsc15/ar_rsc_ar.pkl }

v run on earlier results csv       class: filemodel.resultfile.ResultFile; params: { file: data/rsc15/recommendations/recoms_rsc15_100k_sr }

? run multiple algorithms from a single yml - using key per class (not working)        (memory can be an issue)

v implement Naive 'always yes' option - always return that the item is EOS - on ResultsFile- on FileModel
v implement simple random - to set the probability (* max existing prob to be more 'realistic'  ) - on FileModel

v implement simple random - to set the probability (* max existing prob to be more 'realistic'  ) - on ResultsFile
v implement Naive 'always yes' option - always return that the item is EOS -in rank K = 5- on ResultsFile
? improve Naive 'always yes' to support input rank K

v evaluation support in -1... -n
v push up results

? implement random by probability threshold  (th = 0.1...1)
? implement random by train session len (need to build histogram of the train session lengths)

v Evaluation measures for 1...k (P@k, mrr@k)

? Add print to File

v generate diginetica 0,1,5,10,20,100, aEOS
v generate recsys64 0,1,5,10,20,100, aEOS

v Fix Diginetica 20 (was done with 10)

v aEOS vs random (is aEOS better than random) ; Random on resFile train w/o EOS, test with EOS
v aEOS vs standard (is aEOS much worst on all but EOS) ; aEOS resFile, test w/o EOS
v add FP FN evaluations

v fix  extra sessions with size =2 with aEOS - recsys

fix  extra sessions with size =2 with aEOS- digi

v create results xls

v find in the papers the best Algos to use beside gnn (2)

v Logistic regression
v    LR - AUC
v    LR - P/R graph LR
v Add parameter to do LR
v move LR model to the run config
v Save Pr/Rec - threshhold graph
v save LR prediction  results values in CSV
v Save LR Pres/Recall results values in CSV
v Save PR/RC graph to png file
v calculate HR,FP,FN@0.1...0.9  and save to CSV
v Add LR Results to the out csv BEST hr, auc,
v fix useBothTrainAndTest - avoid using the train data for HR statistics
v Save LR models results

v Save LR models pickle
v Statistical significance pair base vs LR ?

Run tests x DB x Algorithm:
    rsc digi X
        P1 HGRU4Rec, STAMP, SGNN,
        P2 bpr,SR
        P3 NARM,  AR

    EOS
        P1 0 (for later random and naive) 1 10 100
        P2 5 20 sessionLen
        P3 50 200 1000
    [test on train and test - to be able to build CSV that include the train - to build predictions for the LR]

support more datasets
prepare tests for : NaiveTrue,random
support run with a single yml
moving window EOSX

prepare Run in Linux
Save out files location

 ? RunOnAllData - Generate 'complete file model'
 ? EOS1...n

v find in the papers the best Algos to use beside gnn - must not use time value
        Yes: NARM, HGRU4Rec, SGNN, AR (assosication Rule),bpr,Pop,ContextTree, HGRU4Rec(Sort only) , IIRNN,NCFS,Seq2SeqAttNN(STAMP), SR
        No: Using Time: Markov,csrm,SKNN(ContextKNN), STAN,USTAN,VMContextKNN(VSKNN)
        Maybe : ItemKNN (only sort ?)

Session Length distribution

run on recsys64 on all aEOS N --> Find best N aEOS
run on diginetica on all aEOS N --> Find best N aEOS --> N is valid for two datasets
run another algo on recsys --> Find best N aEOS --> N is valid for two algos


Check ensamble

short Run on EC2 GPU - compare runTime
try Colab
Document the run in EC2
load db from web ZIP
Load --> Train --> Run --> save in one command
gen rsc64 dbs with N
gen rsc4 dbs with N
gen more dbs

run all algos on all dbs (w/o aEOS) - reproduce  regular results reported by papers (gen p@1..20  mrr@1...20)
run all algos using N on all dbs (gen p@1..20  mrr@1...20)
run all algos with their saved reults using random (th = 0.1...1)  and (session len) (gen p@1..20  mrr@1...20)

Compare

graphs...
aEOS vs random (is aEOS better than random)
aEOS vs original (how much aEOS fail tha algo)

Check:
Random - set seed = 42 in all places
-------------------------------------------------
Precision / Recall @ K

in regular Session next Click
P@K = TP/(TP+FP) -->
FP= the cases that we didn't manage to predict the next click in the top K == not hit
(hit : 1 no hit: 0) / total
in EOS we can do the same OR
** P@K = hits/(hits + (reported aEOS in none EOS))

R@K = TP/(TP+FN) -->
There are no positive and negative values - there is hit and not hit ; FN=FP=the cases that we didn't manage to predict the next click in the top K == not hit
? Lior: Why they use P@K and R@K in the earlier works ? it's wrong!
in EOS we can do the same OR
** R@K = hits/(hits + (Not reported aEOS in EOS))

How to implement Random
https://dl.acm.org/doi/pdf/10.1145/3109859.3109872
When recurrent neural networks meet the neighborhood for session-based recommendation




