v Remove unneeded changes

v params to preProcess (aEOS 1... N/sessions len, gen dir, name Dir)
v Preprocess 1... N
v Preprocess session len digi and recsys

v enable preprocess recsys

v save model        results:  pickle_models: results/next/rsc15/
v load model        class: filemodel.filemodel.FileModel; params: { modelfile: results/next/rsc15/ar_rsc_ar.pkl }

v run on earlier results csv       class: filemodel.resultfile.ResultFile; params: { file: data/rsc15/recommendations/recoms_rsc15_100k_sr }

? run multiple algorithms from a single yml - using key per class (not working)        (memory can be an issue)

v implement Naive 'always yes' option - always return that the item is EOS - on ResultsFile- on FileModel
v implement simple random - to set the probability (* max existing prob to be more 'realistic'  ) - on FileModel

v implement simple random - to set the probability (* max existing prob to be more 'realistic'  ) - on ResultsFile
v implement Naive 'always yes' option - always return that the item is EOS -in rank K = 5- on ResultsFile
? improve Naive 'always yes' to support input rank K

v evaluation support in -1... -n
v push up results

? implement random by probability threshold  (th = 0.1...1)
? implement random by train session len (need to build histogram of the train session lengths)

v Evaluation measures for 1...k (P@k, mrr@k)

? Add print to File

v generate diginetica 0,1,5,10,20,100, aEOS
v generate recsys64 0,1,5,10,20,100, aEOS

v Fix Diginetica 20 (was done with 10)

v fix  extra sessions with size =2 with aEOS - recsys
fix  extra sessions with size =2 with aEOS- digi

create results xls
find in the papers teh best Algos to use beside gnn (2)
select one more db

recreate all aEOS (1..session Len)
re run algos on results to get more accurate results
update results XLS

run on recsys64 on all aEOS N --> Find best N aEOS
run on diginetica on all aEOS N --> Find best N aEOS --> N is valid for two datasets
run another algo on recsys --> Find best N aEOS --> N is valid for two algos
aEOS vs random (is aEOS better than random) ; Random on resFile train w/o EOS, test with EOS
aEOS vs standard (is aEOS much worst on all but EOS) ; aEOS resFile, test w/o EOS

fix Precision and Recall (multiple)

short Run on EC2 GPU - compare runTime

try Colab

Document the run in EC2
load db from web ZIP
Load --> Train --> Run --> save in one command


gen rsc64 dbs with N
gen rsc4 dbs with N
gen more dbs

run all algos on all dbs (w/o aEOS) - reproduce  regular results reported by papers (gen p@1..20  mrr@1...20)
run all algos using N on all dbs (gen p@1..20  mrr@1...20)
run all algos with their saved reults using random (th = 0.1...1)  and (session len) (gen p@1..20  mrr@1...20)

Compare

graphs...
aEOS vs random (is aEOS better than random)
aEOS vs original (how much aEOS fail tha algo)

Check:
Random - set seed = 42 in all places
-------------------------------------------------
Precision / Recall @ K

in regular Session next Click
P@K = TP/(TP+FP) -->
FP= the cases that we didn't manage to predict the next click in the top K == not hit
(hit : 1 no hit: 0) / total
in EOS we can do the same OR
** P@K = hits/(hits + (reported aEOS in none EOS))

R@K = TP/(TP+FN) -->
There are no positive and negative values - there is hit and not hit ; FN=FP=the cases that we didn't manage to predict the next click in the top K == not hit
? Lior: Why they use P@K and R@K in the earlier works ? it's wrong!
in EOS we can do the same OR
** R@K = hits/(hits + (Not reported aEOS in EOS))

How to implement Random
https://dl.acm.org/doi/pdf/10.1145/3109859.3109872
When recurrent neural networks meet the neighborhood for session-based recommendation




